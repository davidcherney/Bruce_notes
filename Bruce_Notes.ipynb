{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5c87ba-d1c9-437e-ae51-8d1a20d32ec3",
   "metadata": {},
   "source": [
    "# 1: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc6723d-fd39-41c4-a533-757fb987c4be",
   "metadata": {},
   "source": [
    "Pandas has a `cut` method to create bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe289f-fd65-4335-a74b-166327dff939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "data = pd.DataFrame(columns=diabetes['feature_names'], data = diabetes['data'])\n",
    "s3s = pd.cut(data['s3'], 10) # assigns an interval to each index\n",
    "s3s.value_counts().sort_index() # sort index sorts on start of interval here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99103ac3-57c2-4af2-af1b-96f9b949bea9",
   "metadata": {},
   "source": [
    "Pyplot's histogram is actually this same information organized differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2b6b3-7064-4f71-8c2d-10408e5d8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "h = plt.hist(data['s3'], bins=10);\n",
    "counts = [int(x) for x in h[0] ]\n",
    "cutoffs = [round(x,3) for x in h[1]] # cutoffs\n",
    "for i in range(len(h[0])):\n",
    "    print(f'( {cutoffs[i]}, {cutoffs[i+1]} ]    {counts[i]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a192406-1955-4d7d-b251-2b3554a54936",
   "metadata": {},
   "source": [
    "Pandas supports smoothing of histograms to approx density (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b0961-52d4-4bf5-9199-1ab22d513ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.density(data['s3']); #DNE\n",
    "data['s3'].plot.density()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78726a65-81a1-4b3f-b08a-bd30f5df8ff4",
   "metadata": {},
   "source": [
    "## Estimates of Location \n",
    "\n",
    "**Timmed mean** is the mean of the est with the highest 10% and lowest 10% removed... this is to reduce sensativity to outliers. Pandas doesn't have it, scipy.stats does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81d2c3-6cb7-49e7-bf49-a6fd8e3005e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c16b9-5bb6-4f57-83c6-86c49cf127fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.trim_mean(data['s3'], \n",
    "                proportiontocut= .1, # there is no default here\n",
    "               ), data['s3'].mean() # to see that they are not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b097c9c-5738-4ef4-9203-10ef80660bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check that this is what I expect it to be.. I'll come up with a \n",
    "# alternative with pandas quantile function:\n",
    "low = data['s3'].quantile(.1)\n",
    "high = data['s3'].quantile(.9)\n",
    "\n",
    "fil = (data['s3']>=low) & (data['s3']<=high)\n",
    "\n",
    "( data['s3'][ fil]).mean() # dang it... why is this not the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd256c-6452-4992-81fc-b85a34dbb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eed2da-07e8-43c1-a627-434724f0d1b4",
   "metadata": {},
   "source": [
    "## Estimates of Variablility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e605f-46f1-4ccf-acfb-893ea7f29964",
   "metadata": {},
   "source": [
    "**Mean Absolute deviation from the median**\n",
    "$\\text{mean}\\left\\{ |x_i -m| \\right\\}$ where\n",
    "$m = \\text{median}\\{x_i\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b11b1c-feeb-4c97-8ab6-d89bd5abcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data['s3'].median()\n",
    "devs = data['s3'] - m\n",
    "abdevs = devs.map(abs)\n",
    "mad = abdevs.mean()\n",
    "mad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b11ed-e446-431d-be61-0e9483c766f3",
   "metadata": {},
   "source": [
    "**Median Absolute Deviation from the Median**\n",
    "$\\text{median}\\left\\{ |x_i -m| \\right\\}$ where\n",
    "$m = \\text{median}\\{x_i\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e39179-8302-49e9-9ec2-776a458965cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data['s3'].median()\n",
    "devs = data['s3'] - m \n",
    "abdevs = devs.map(abs)\n",
    "mead = abdevs.median()\n",
    "mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e75d8-2127-432e-99c5-539e0ed3b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['s3'], alpha =0.5)\n",
    "plt.vlines(x=[m-mad,m, m+mad], ymin=0, ymax =100, color='salmon', alpha = 0.9, label ='MAD')\n",
    "plt.vlines(x=[m-mead,m, m+mead], ymin=0, ymax =90, color='red', alpha = .9, label= 'MeaAD')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eac961-a29e-4bbf-afec-72ec5b6a2c8c",
   "metadata": {},
   "source": [
    "## Exploring 2 or more variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27595acb-99f6-4070-811a-c7fb8a28778e",
   "metadata": {},
   "source": [
    "When there are so many points to plot that it is visually overwhelming, it is nice to use either contour plots or hexbin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123a060-d94a-4ec6-b9db-b532f9260add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is a method from pandas plot library. \n",
    "data.plot.hexbin(x='s3', y='s2', \n",
    "                 gridsize=30,\n",
    "                );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b39cd-a3b4-432e-8c92-05d1f439d3ca",
   "metadata": {},
   "source": [
    "When you have two categorical variables, you might want a table of counts of things in two categories. That is a kind of pivot table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcb810-16ab-47c4-bc1f-7193a2139b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "c_feats = [ 'MS Zoning', 'Lot Shape'] \n",
    "dfc = df[c_feats]\n",
    "\n",
    "dfc.pivot_table(index = 'MS Zoning', columns = 'Lot Shape', \n",
    "               aggfunc = lambda x : len(x),\n",
    "               margins = True,\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273964f-ccff-41a3-b684-27c8e78f85cf",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737142e0-f3ba-494e-8185-6151181d8f5a",
   "metadata": {},
   "source": [
    "There are other types of correlation coefficient, since Pearson's has some flaws. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d060a6e-70b3-4308-95a9-f0d69c41473d",
   "metadata": {},
   "source": [
    "### Spearman's rho \n",
    "$\\rho$\n",
    "\n",
    "Given two lists of the same length of values of observables\n",
    "- give the values of the first a ranking\n",
    "- give the values of the second a ranking, \n",
    "- calculate the sum of the squares of the distances between rankings\n",
    "- $\\rho = 1-\\frac{6\\sum d^2}{n(n^2 -1)}$\n",
    "- this is the pearson correlation between the two rank series. \n",
    "- Spearman's $\\rho$ is thus a measure of monotonicity of $x$ vs $y$, regardless of the linearity. \n",
    "- I also not that the original order of the points is forgotten in Spearman, but not Pearson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dddeaf-f8ac-459d-b9a3-2d5dd664fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "df = pd.DataFrame(data = diabetes.data, columns= diabetes.feature_names)\n",
    "df['s2'].rank()[:3] # there is a nice method to give the rank of each entry, with averaging :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc282139-9868-4e94-8083-2e1da30738d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dsq'] = (df['s2'].rank() - df['s3'].rank())**2\n",
    "n = len(df)\n",
    "1-6*df['dsq'].sum()/(n*(n+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8de65-c6c3-4409-a3e1-4ed9a7c0f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(df['s2'],df['s3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290ba81-67ae-486c-baeb-9f4a03359854",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(df['s2'].rank(), df['s3'].rank())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae8344-4f36-4cc9-8cfb-839584f733c2",
   "metadata": {},
   "source": [
    "### Kendal's tau \n",
    "\n",
    "Let $a_{ij}  = {\\text{Sign}} (x_i-x_j)$ and Let $b_{ij} = \\text{sign}(y_i-y_j)$. \n",
    "\n",
    "Then Kendal's $\\tau = \\frac{\\langle a,b \\rangle }{\\lVert a\\rVert \\lVert b \\rVert}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586060e3-ef7b-4d06-b5cd-3f35d5498689",
   "metadata": {},
   "source": [
    "This is the normalized count of concordant pairs: $(x_1,y_y), ~(x_2,y_2)$ such that $the order of $x_1,x_2$ is the same as the order of $y_1,y_2$.\n",
    "\n",
    "There is a generalized form of this for any antisymmetric matrix $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad8b69-635a-4b64-9753-e1d1f7f87955",
   "metadata": {},
   "source": [
    "## Categorical and Numeric Data (tegether)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f2933-fc9c-46fc-9412-039fc6aa3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas has a nice way to show box plots of a quantity for each value of a categorical vbariable. \n",
    "df.boxplot(by=c_feats[0], column = 'Lot Area');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0d026-7896-424a-b668-9d3a1d154f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh # looks badass for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293235c-f4c2-4e4b-8cf5-e89b61e48d7b",
   "metadata": {},
   "source": [
    "# 2: Data and Sampling Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87ccf5-69cc-45f7-898f-bcd82f141846",
   "metadata": {},
   "source": [
    "**The vast search effect**\n",
    "\n",
    "If you torture a dataset long enough it will eventually confess... but that confession is not trustworthy. Running lots of models and asking lots of question is not always good. \n",
    "\n",
    "**Target Shuffling** is a data science approach to significance testing: you found a good model, lovely. Record its $R2$ score (as an example) and then\n",
    "- shuffle the target data\n",
    "- run the fit\n",
    "- record whether you got a $R^2$ score better than that or not. \n",
    "- repeat 100, or 100,000 times. \n",
    "- What fraction of the times do you get a better result? 5%? Then there is a 5% chance that your good model came from chance alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf2f73-2896-4f3d-8602-25ea19222612",
   "metadata": {},
   "source": [
    "**Samipling error** is the term for standard deviation of a sampling distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c4f83-71ec-4184-b6a3-d3411767cd4c",
   "metadata": {},
   "source": [
    "## Bootsraping\n",
    "is the modern way to estimate confidence intervals. \n",
    "\n",
    "Note that the most intuitive interpretation of a confidence interval \"95% probability that the true value lies within...\" is wrong. THe correct wording must start with \"given a sampling procedure and a population, what is the probability that\". \n",
    "\n",
    "I believe the correct interpretation is \"if we sampled this population 100 times, 95% of the time we'd get intervals that include the population mean 95% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b76033-e4ab-4924-99da-fde1a9542a95",
   "metadata": {},
   "source": [
    "## Prediction Intervals vs CIs\n",
    "\n",
    "PIs \n",
    "- are about where you expect future values to fall\n",
    "- aren't where you expect the population mean to be. \n",
    "- are always bigger than CIs since ngs must be taken into account:\n",
    "    1. The variation in estimating the population mean from a sample (the standard error)\n",
    "    2. The variation in the observation around the mean. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c66e61-bf99-4202-8008-a7639b66e358",
   "metadata": {},
   "source": [
    "Following <a href=\"https://towardsdatascience.com/confidence-intervals-vs-prediction-intervals-7b296ae58745\">Michal</a>\n",
    "- calculate PI and CI using boostrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a2f89-68cd-4177-8f06-df5bc18c67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(data = housing['data'], columns = housing['feature_names'])\n",
    "df['target'] = housing['target'] \n",
    "df = df[['MedInc','target']].sample(200) # two columns, 200 rows\n",
    "\n",
    "# df.head(3)\n",
    "\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "\n",
    "mu_dist = list()\n",
    "pred_dist = list()\n",
    "\n",
    "for _ in range(1_000):\n",
    "    sample = df.sample(len(df), replace=True)\n",
    "    lr.fit(sample[['MedInc']], sample['target'])\n",
    "    \n",
    "    pred = lr.predict(pd.DataFrame({'MedInc':[3]}))[0] #prediction for 3\n",
    "\n",
    "    # calculate residuals\n",
    "    preds = lr.predict(sample[['MedInc']][:1]) \n",
    "    residuals =  (sample['target'] - preds)\n",
    "\n",
    "    mu_dist.append(pred)\n",
    "    pred_dist.append(pred+ residuals.sample(1))\n",
    "                                               \n",
    "print(f'{round(np.mean(mu_dist),2)}, CI = {np.percentile(mu_dist, [5,95] )} ')\n",
    "print(f'{round(np.mean(pred_dist),2)}, PI = { np.percentile(pred_dist, [5,95])} ')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ba8b3-651a-4417-aba1-6ce89bfd396c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82077b0b-c810-438e-8aab-189b5818f31f",
   "metadata": {},
   "source": [
    "# Q-Q plot \n",
    "**Handmade:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f76cf1-beb0-4ff3-b3a1-b8e6ce5c37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69a6ae-5a7f-47ba-b3f2-2f79ff26fb7b",
   "metadata": {},
   "source": [
    "Experiment: Flip a coin 2,000 times. \n",
    "\n",
    "Wanted: 1000 observations of the number of heads in 2,000 flips.  \n",
    "\n",
    "i.e. 1,000 samples from a n=2000 binomial distributionn with P=.53. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903463a0-d0f6-482e-9c49-eb5fc2a0512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "samples = np.random.binomial(n, 0.53, size = 1000)\n",
    "# samples.shape # (1000,)\n",
    "# samples[:20] #each of these is the sum of a sample, ~ of size 0.53*2000 = 1033\n",
    "# lets look at the p values of the samples, they should be p close to P=0.53\n",
    "ps = samples/n\n",
    "# ps[:20] # yup\n",
    "\n",
    "# standardize the observation\n",
    "zs = (ps-np.mean(ps))/np.std(ps)\n",
    "zs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949b780-5a78-4667-b597-8e3777d76793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I get quantiles of a list? np has quantile\n",
    "xs = np.linspace(0,1,100)\n",
    "qs = np.quantile(zs,xs) # quantiles of my distribution\n",
    "plt.scatter(stats.norm().ppf(xs), qs, label= 'QQ',alpha=0.5) \n",
    "plt.plot(stats.norm().ppf(xs),stats.norm().ppf(xs), color='salmon',label='line')\n",
    "plt.xlabel('Normal')\n",
    "plt.title(\"Handmade QQ Plot\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94146e27-15c6-4f5f-86bc-1e94dcb256f2",
   "metadata": {},
   "source": [
    "**Implementation using scipy.stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296954b-f2b7-4312-b740-348aaaae387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(qs, #plot the quantiles of these standardized means of samples\n",
    "               dist=\"norm\", # norm is default, this sets the x axis \n",
    "               plot=plt) # show the plot\n",
    "plt.title(\"Normal Q-Q plot\");\n",
    "# the binning seems to be quite different than what I chose. \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcdabb-9877-41c3-9a71-fe1002870f0e",
   "metadata": {},
   "source": [
    "**Implementation using statsmodels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bfded-3293-477d-b496-b5f7398f624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import statsmodels.api as sm \n",
    "import pylab as py \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcf5fd-1cbe-4058-9e4a-70049c0c018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random generates different random numbers everytime the code is executed.\n",
    "data_points = np.random.normal(0, 1, 100)     \n",
    "  \n",
    "sm.qqplot(data = zs, # data_points, \n",
    "          # dist = idk how to tell it, but normal is default\n",
    "          line ='45',\n",
    "         ) \n",
    "py.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdfc5a-2c7a-4309-a04f-926765f3d7e9",
   "metadata": {},
   "source": [
    "This looks off\n",
    "\n",
    "what the heck yo? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73338b-319f-4b44-8d54-31d20e185d7f",
   "metadata": {},
   "source": [
    "## Poisson vs Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec658085-fbd8-4571-8e98-be9c00fb147d",
   "metadata": {},
   "source": [
    "The Poisson disribution is discrete and interepreted as the number of events per unit time when there is an expected number of events pre unit time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c4b2a-542d-4cd6-b04f-97fbaba77bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db1bf5-25b8-46aa-8cc5-d5dd55a7eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stats.poisson(mu = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ebad1-36a4-4011-9b02-e51ff006be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.rvs(size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf669fad-fc68-4dff-b7c9-aa4505d46cc0",
   "metadata": {},
   "source": [
    "By contrast, the exponential distribution is continuous and give the time between events when the number of events per unit time is known. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21ef09-232c-414f-a3b4-76edf74bae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = stats.expon(scale = 2)\n",
    "Y.rvs(size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bdfa4-1092-47ee-84a7-093ba9ca9494",
   "metadata": {},
   "source": [
    "## Weibill Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502adaf5-1db6-49b0-ab1d-c52e92d18559",
   "metadata": {},
   "source": [
    "has a very nice CDF \n",
    "$$\n",
    "F(x) = 1-e^{-\\left(\\frac{x}{\\eta} \\right)^a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3928481-5309-43b6-9c5f-d323c4afd42d",
   "metadata": {},
   "source": [
    "and is very similar to the logistic CDF (by virtueo f being the Taylor approx?)\n",
    "$$\n",
    "F(x) = \\frac{1}{1+e^{-a(x-\\eta) }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fda4d-d789-4615-9d82-bb633e5e6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "a,loc = 3, 5\n",
    "X = stats.weibull_min(c=a, loc=loc)\n",
    "Y = stats.logistic(loc=loc+1, scale=1/a)\n",
    "\n",
    "xs = np.linspace(0,10,100)\n",
    "ysl = Y.cdf(xs)\n",
    "ysw = X.cdf(xs)\n",
    "\n",
    "plt.plot(xs,ysw, label='Weibel')\n",
    "plt.plot(xs,ysl, label = 'Logistic')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ce62b-6fdf-4a7e-97aa-52cf03ce71df",
   "metadata": {},
   "source": [
    "Wind speed distributions are best modeled as Weibill distributions, and are less accurately approximated by Rayleigh (the $a=2$ case.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bc25e-0fb8-4d84-93e4-c8ade314d992",
   "metadata": {},
   "source": [
    "idk what the max vs min is about. Apparently the dist is for max and min of iid rand vars.\n",
    "<a href = 'https://github.com/scipy/scipy/issues/10014'>see</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3d80c-acae-4d05-a264-a7e646244981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weibill max = - Weibill min\n",
    "M = stats.weibull_max(c=a, loc=loc)\n",
    "ysm = M.cdf(xs)\n",
    "plt.plot(xs,ysm, alpha =0.5)\n",
    "plt.plot(xs,ysw, alpha = .5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5661eb2-a0dc-4144-9226-b0dc0efa8043",
   "metadata": {},
   "source": [
    "## The Gumbel dsitribution\n",
    "is nice for modeling time at failure of things that tend to die at a certain time.\n",
    "$$\n",
    "f(T) =\\frac1\\sigma e^{-e^{(T-\\mu)/\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd0b7d-0f8e-439d-a746-3b97c207e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = stats.gumbel_r(loc=loc, scale =1/a)\n",
    "ysg = Z.cdf(xs)\n",
    "plt.plot(xs,ysg);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1b4ec-24ce-4e4f-b8f4-168567e234ae",
   "metadata": {},
   "source": [
    "# ch 3 Statistical Experiments`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
